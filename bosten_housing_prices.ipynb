{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bosten Housing Prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## General\nHousing Values in Suburbs of Boston. The **medv** variable is the target variable. <br>\n\n![](http://www.houseandhammer.com/wp-content/uploads/2016/11/11cudworthmedford-750x429.jpg)\n\n\n## Data description\nThe Boston data frame has 506 rows and 14 columns.This data frame contains the following columns:<br>\n<ul>\n <li>\n     **crim**: per capita crime rate by town.\n </li>\n <li>\n     **zn**: proportion of residential land zoned for lots over 25,000 sq.ft.\n </li>\n <li>\n     **indus**: proportion of non-retail business acres per town.\n </li>\n <li>\n     **chas**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n </li>\n <li>\n     **nox**: nitrogen oxides concentration (parts per 10 million).\n </li>\n <li>\n     **rm**:average number of rooms per dwelling.\n </li>\n <li>\n     **age**: proportion of owner-occupied units built prior to 1940.\n </li>\n <li>\n     **dis **: weighted mean of distances to five Boston employment centres.\n </li>\n <li> \n     **rad**: index of accessibility to radial highways.\n </li>\n <li>\n     **tax**: full-value property-tax rate per  \\$10,000.\n </li>\n <li>\n **ptratio **: pupil-teacher ratio by town.\n </li>\n <li>\n **black **: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n </li>\n <li>\n **lstat **: lower status of the population (percent).\n </li>\n <li>\n **medv **: median value of owner-occupied homes in \\$1000s.\n</ul>\n\n **Source **\nHarrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.\nBelsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n\n\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#1.1Step: Import libraries which are necessary for this project\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom IPython.core.display import display, HTML\n\n\n#1.3.Step: For a nice display in the notebooks\n%matplotlib inline\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#1.4.Step: Preprocessing\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nboston.feature_names \nlower_case = lambda x:x.lower()\nfunc_lower = np.vectorize(lower_case)\nfeature_names = func_lower(boston.feature_names)\nprint(feature_names)\nboston = load_boston()\ndf_boston = pd.DataFrame(boston.data, columns=feature_names)\ndf_boston['medv'] = pd.Series(boston.target)\ndf_boston.head()\n\ndisplay(df_boston.describe())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\ncorr = df_boston.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=100),\n    annot=True, \n    fmt='.2g',\n    square=True,\n\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\nplt.show()\n\ncorrelations = df_boston.corr()\ncorrelations = correlations['medv']\ndisplay(HTML('''\n<p>For the first try the Pearson correlation will be used to measure the corrleation. The Pearson coefficient has values between -1 to 1 </p>\n<ul>\n    <li>0: A value close to zeroe imples a weak correlation. A value of exact 0 impliess no correlation.</li>\n    <li>-1: a value close to -1 implies a strong negativ correlation</li>\n    <li>+1: a value close to +1 implies a strong postiv correlation</li>\n</ul>\n\n'''))\n\ndisplay(\n    correlations.sort_values()\n)\n\n#Step: Selecting only the relevant features\n\nrelevant_feat = correlations[(correlations>0.5) | (correlations<-0.5)]\n\ndisplay(HTML(\"\"\"<h3> Relevant features </h3>\"\"\"))\ndisplay(relevant_feat)\n\ndisplay(\n    HTML(\"\"\"\n        <h3> Finding </h3>\n        You can see that feature's: <br>\n        <ul>\n        <li>lstat  [lower status of the population (percent)] \n        </li><li>\n        rm (average number of rooms)\n        </li><li>\n        ptratio (pupil-teacher ratio by town. )\n        </li>\n        </ul>\n        correlates quite strongly to the medv. However  inorder to be ableu the use Pearson Correlation the <br>\n        the following assumptions need to be checked\n        <ol>\n            <li>continuous variables</li>\n            <li>Cases that have values on both variables</li>\n            <li>Linear relationship between the variables</li>\n            <li>Independent cases (i.e., independence of observations) </li>\n            <li>Bivariate normality </li>\n            <li>Random sample of data from the population</li>\n            <li>No outliers</li>\n        </ol>\n        Source: https://libguides.library.kent.edu/SPSS/PearsonCorr\n        \n        \"\"\"\n        )\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#todo: Write the reasons\n\ndisplay(\n    HTML(\"\"\"\n        <h3> Check  linear assumptions </h3>\n        <ol>\n            <li>continuous variables <br> <b>Fulfilled </b>: </li>\n           \n            <li>Cases that have values on both variables <br> <b>Fulfilled </b></li>\n            <li>Linear relationship between the variable s<br> <b>Fulfilled </b></li>\n            <li>Independent cases (i.e., independence of observations)<br> <b>need to be investigated further </b></li>\n            <li>Bivariate normality <br> <b>need to be investigated further </b></li> </li>\n            <li>Random sample of data from the population <br> <b>Fulfilled </b></li>\n            <li>No outliers <br> <b>need to be investigated further </b> </li>\n        </ol>\n        Source: https://libguides.library.kent.edu/SPSS/PearsonCorr\n        \n        \"\"\"\n        )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\"\"\"\n<h5>Let explore the correlation between the independent variables </h5>\n\"\"\"))\nplt.figure(figsize=(5,5))\ncorr = df_boston[[\"lstat\",\"rm\",\"ptratio\"]].corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=plt.cm.Reds,\n    annot=True, \n    fmt='.2g',\n    square=True,\n\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\nplt.show()\n\n\ndisplay(HTML(\"\"\"\n<h5>Findings</h5>\n<P>As we can see the RM and LSTAT are strongly correlated with each other, <br>\ntherefore I will only keep LSTAT because its correlation coefficient with medv is higher. I will also keep PTRATIO <br>\nbecause the PC value is below 0.5</p>\n\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\n\"\"\"\n<h3>Multilinear regression</h3>\n<p>Prior I identify the most relevant feature to determine the target feature (medv). In this section I will build a \nmultilinear regression, in order to find out how good the identify variable explaine the target var.</p>\n\"\"\"\n))\n\n#todo: Complete this section\n\ndisplay(HTML(\n\"\"\"\n<h3>Multilinear regression</h3>\n\n\n<p>Find a a multilinear model based on: </p>\n<ol>\n    <li>Just use all features \n    \n    </li>\n    <li>Backwards Elimination\n        <ol>\n            <li>Fit model with all possible features</li>\n            <li>Order the features according the P-Value</li>\n            <li>Remove the feature with the highest P-Value, if the p-value is higher than 5%. <br> If\n            no P-Value is higher than 5% the model is finished</li>\n            <li>Refit the model, and go two steps back</li>\n        </ol>\n    </li>\n    <li>Forward Selection\n        <ol>\n            <li>#Uncomplete</li>\n        </ol>\n    </li>   \n    <li>Bidirectional Selection\n        <ol>\n            <li>#Uncomplete</li>\n        </ol>\n    </li>  \n    <li>Sorce Comparision\n        <ol>\n            <li>#Uncomplete</li>\n        </ol>\n    </li>  \n</ol>\n\"\"\"\n))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y = df_boston.iloc[:,-1]\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\n\"\"\"\n<h5>Backwards Elimination </h5>\n\"\"\"))\n\n#Split data into dependent and indepemente var`s\nX = df_boston.iloc[:,:-1]\ny = df_boston[[\"medv\"]]\n#Splitting the dataset into Training and Test set \nfrom sklearn.cross_validation import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n\ndisplay(HTML(\n\"\"\"\n<h5>There is no need to scale the feature var`s because it will be done by the lib because we use multilineare regression</h5>\n\"\"\"))\n\n\n#Fitting the multiple linear regression to the created training data\nimport math \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score,explained_variance_score\n\n\nmodel_all = LinearRegression()\nmodel_all.fit(X_train,y_train)\n\n\n#Evaluate the created model\ny_pred = model_all.predict(X_test)\nplt.style.use('bmh')\n\n#Explore the data\nfig = plt.figure()\n#===== Model with all features \ndf_visu = pd.DataFrame(data=y_pred,columns=[\"predicted\"])\n\ndf_visu_all= pd.concat([\n                        df_visu,y_test.reset_index(drop=True)\n                       ], \n                        axis=1, sort=False\n                       )\n\ndf_visu_all.plot.line(figsize=(20,10),title='Model with all feature',\n                            marker=\".\")\n\nplt.show()\ndisplay(HTML(\n\"\"\"\n<h4>Finding as </h4>\"\"\"))\n# The coefficients\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_pred, y_test))\nprint(\"Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred, y_test)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_pred, y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as sm\n\n\nres = mod.fit()\nprint(res.summary())\n\nfeature_name = df_boston.keys().tolist()\nfeature_name.remove('medv')\nprint(feature_name)\n\nres\nnumVars = len(feature_name)\nsl=0.05\nfor i in range(0, numVars):\n    #1.Step: Building the string\n    str_formular='medv ~'\n    feature_str= ''\n    for element in feature_name:\n        feature_str+=\" + \"+element\n    str_formular+=feature_str\n    print(str_formular.replace(\"~+\",\"~\"))\n    #2.Step:Building model \n    mod = sm.ols(formula=str_formular, data=df_boston)\n    res_back = mod.fit()\n    maxVar=res_back.pvalues.max()\n\n    if maxVar > sl:\n        name_var = res_back.pvalues.idxmax()\n        feature_name.remove(name_var)\n        print(\"#\"*10)\n        print(name_var)\n        print(\"#\"*10)\n    res_back.summary()\n\n\n\n\n#Evaluate the created model\ny_pred_all = model_all.predict(X_test)\ny_pred_select = res.predict(X_test)\nplt.style.use('bmh')\n\n#Explore the data\nfig = plt.figure()\n#===== Model with all features \ndf_visu_all = pd.DataFrame(data=y_pred_all,columns=[\"predicted_all\"])\ndf_visu_select = pd.DataFrame(data=y_pred_select,columns=[\"predicted_select\"])\ndf_visu= pd.concat([\n                        df_visu_all,\n                        df_visu_select.reset_index(drop=True),\n                        y_test.reset_index(drop=True)\n                       ], \n                    axis=1, sort=False\n                   )\n\ndf_visu.plot.line(figsize=(20,10),title='Model with all feature',\n                            marker=\".\")\n\nplt.show()\ndisplay(HTML(\n\"\"\"\n<h4>Finding  </h4>\"\"\"))\n# The coefficients\nprint(\"All: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_all, y_test))\nprint(\"Select: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_select, y_test))\nprint(\"All: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_all, y_test)))\n# Explained variance score: 1 is perfect prediction\nprint(\"Select: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_select, y_test)))\nprint('All: Variance score: %.2f' % r2_score(y_pred_all, y_test))\n# Explained variance score: 1 is perfect prediction\nprint('Select: Variance score: %.2f' % r2_score(y_pred_select, y_test))\n\n\ndisplay(HTML(\n\"\"\"\n<h5>As we can see, even if we use multply linear regression the result is still quite bad, <br>\nthis could mean that the is some non linear correlation </h5>\"\"\"))\n\n## On","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_self ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\n\"\"\"\n<h5>Now I will build a multi. linear model with just the identify feature </h5>\"\"\"))\n\nmod_self = sm.ols(formula=\"medv ~ lstat +  ptratio \", data=df_boston)\nres_self = mod_self.fit()\nres_self.summary()\n\n\n\n\n#Evaluate the created model\ny_pred_all = model_all.predict(X_test)\ny_pred_select = res.predict(X_test)\ny_pred_self = res_self.predict(X_test)\nplt.style.use('bmh')\n\n#Explore the data\nfig = plt.figure()\n#===== Model with all features \ndf_visu_all = pd.DataFrame(data=y_pred_all,columns=[\"predicted_all\"])\ndf_visu_select = pd.DataFrame(data=y_pred_select,columns=[\"predicted_select\"])\ndf_visu_self = pd.DataFrame(data=y_pred_self,columns=[\"predicted_self\"])\ndf_visu= pd.concat([\n                        df_visu_all,\n                        df_visu_self.reset_index(drop=True),\n                        df_visu_select.reset_index(drop=True),\n                        y_test.reset_index(drop=True)\n                       ], \n                    axis=1, sort=False\n                   )\n\ndf_visu.plot.line(figsize=(20,10),title='Model with all feature',\n                            marker=\".\")\n\nplt.show()\ndisplay(HTML(\n\"\"\"\n<h4>Finding  </h4>\"\"\"))\n# The coefficients\nprint(\"All: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_all, y_test))\nprint(\"Select: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_select, y_test))\nprint(\"Self: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_self, y_test))\nprint(\"All: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_all, y_test)))\n# Explained variance score: 1 is perfect prediction\nprint(\"Select: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_select, y_test)))\nprint(\"Self: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_self, y_test)))\nprint('All: Variance score: %.2f' % r2_score(y_pred_all, y_test))\n# Explained variance score: 1 is perfect prediction\nprint('Select: Variance score: %.2f' % r2_score(y_pred_select, y_test))\nprint('Self: Variance score: %.2f' % r2_score(y_pred_self, y_test))\n\ndisplay(HTML(\n\"\"\"\n<h5>As we can see, even if we use multply linear regression the result is still quite bad, <br>\nthis could mean that the is some non linear correlation </h5>\"\"\"))\n\n## On","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"display(HTML(\"\"\"\n<h5>Let’s use pandas to visualise this correlation further</h5>\n\"\"\"\n))\n\n#Explore the data\nplt.subplots_adjust(left=0.2, bottom=None, right=None, top=None, wspace=None, hspace=1)\nplt.subplot(211)\n#===== AGE\nplt.subplot(2, 1, 1)\ndf_boston['age'].plot.hist(title='Proportion of owner-occupied units built prior to 1940',\n                            colormap='jet')\nplt.xlabel('age')\nplt.ylabel('frequency')\n#===== \nplt.subplot(2, 1, 2)\ndf_boston['crim'].plot.hist(title=\"per capita crime rate by town\",\n                           colormap=\"jet\"\n                           )\nplt.xlabel(\"crime\")\nplt.ylabel(\"frequency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom pandas.plotting import scatter_matrix\n\ncolumns_visual=['rm','lstat','ptratio']\n\nfor col_name in columns_visual:\n    plt.figure()\n    df_boston.plot.scatter(x=col_name, y='medv', title=\"Medv and {col_name}\".format(col_name=col_name))\n\n\n\nscatter_matrix(df_boston[columns_visual],figsize=(15,12),alpha=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creation a prediciton model\n\nIn order to mesaure the quality of the developed model I will you R². The Range of this coefficient  is between 0 to 1. \n\n- 0: A model with a R² is as good as a model which always predict the mean of a traget variable.\n- 1: The developed model can perfely predict the target varaible\n- 0-1: Can be see as an indicator, what a percentag the target variable can be explaind by the feature which this model uses. \n  "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Import 'r2_score'\n\nfrom sklearn.metrics import r2_score\n\n# Split date into training and test data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_boston[columns_visual], df_boston['medv'], test_size=0.2, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pydotplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit\n\ndef fit_model(X, y):\n    \"\"\" Performs grid search over the 'max_depth' parameter for a \n        decision tree regressor trained on the input data [X, y]. \n    Source:https://towardsdatascience.com/machine-learning-project-predicting-boston-house-prices-with-regression-b4e47493633d     \n    \"\"\"\n    \n    # Create cross-validation sets from the training data\n    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n\n    # Create a decision tree regressor object\n    regressor = DecisionTreeRegressor()\n\n    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n    params = {'max_depth':[1,2,3,4,5,6,7,8,9,10]}\n\n    # Transform 'performance_metric' into a scoring function using 'make_scorer' \n    scoring_fnc = make_scorer(r2_score)\n\n    # Create the grid search cv object --> GridSearchCV()\n    grid = GridSearchCV(estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_,grid\n\n# Fit the training data to the model using grid search\nreg,grid = fit_model(X_train, y_train)\n\n# Produce the value for 'max_depth'\nprint(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"grid.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create test data to explore the created model \n\n#             rm ,lstat,ptratio   \nimaginary_data = [[2, 15, 20], \n               [3, 25, 26],\n               [6, 5, 9]]  \n\n# Show predictions\nfor i, price in enumerate(reg.predict(imaginary_data)):\n    print(\"Predicted selling price for test data {}'s : ${:,.2f}\".format(i+1, price))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\n\n# https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176\ndot_data = StringIO()\n\nexport_graphviz(reg, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model’s Sensitivity\n\nAfter I found an optimal model I will check the robustness of the developed model. \nIt can happen that the optional model is overfiting order underfitting. \n\nOne way to check if the model is robistness of a concept is to created differnt models and see how far the spread is for the same data set ( I will check agains the imaginary_data).\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.Step Change the test_split to \ni=0\ndict_model={}\ndf_sens= pd.DataFrame(data={\"name\":[],\"predict_value\":[]})\nfor size_test_split in [0.1,0.15,0.22,0.27]:\n    \n    X_train, X_test, y_train, y_test = train_test_split(df_boston[columns_visual], df_boston['medv'], test_size=0.2, random_state = 42)\n    reg,grid = fit_model(X_train, y_train)\n    dict_model[i]={\"reg\":reg,\"grid\":grid}\n    df_sens = df_sens.append({\"name\":i,\"predict_value\":reg.predict([[2, 15, 20]])[0]},ignore_index=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df_sens.append({\"name\":i,\"predict_value\":reg.predict([[2, 15, 20]])[0]},ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_sens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_boston.plot.scatter(x=df_sens[\"predict_value\"], y=df_sens[\"predict_value\"], title=\"Medv and rm\")\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python (python35)","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"}},"nbformat":4,"nbformat_minor":1}