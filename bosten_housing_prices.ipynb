{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bosten Housing Prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## General\nHousing Values in Suburbs of Boston. The **medv** variable is the target variable. <br>\n\n![](http://www.houseandhammer.com/wp-content/uploads/2016/11/11cudworthmedford-750x429.jpg)\n\n\n## Data description\nThe Boston data frame has 506 rows and 14 columns.This data frame contains the following columns:<br>\n<ul>\n <li>\n     **crim**: per capita crime rate by town.\n </li>\n <li>\n     **zn**: proportion of residential land zoned for lots over 25,000 sq.ft.\n </li>\n <li>\n     **indus**: proportion of non-retail business acres per town.\n </li>\n <li>\n     **chas**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n </li>\n <li>\n     **nox**: nitrogen oxides concentration (parts per 10 million).\n </li>\n <li>\n     **rm**:average number of rooms per dwelling.\n </li>\n <li>\n     **age**: proportion of owner-occupied units built prior to 1940.\n </li>\n <li>\n     **dis **: weighted mean of distances to five Boston employment centres.\n </li>\n <li> \n     **rad**: index of accessibility to radial highways.\n </li>\n <li>\n     **tax**: full-value property-tax rate per  \\$10,000.\n </li>\n <li>\n **ptratio **: pupil-teacher ratio by town.\n </li>\n <li>\n **black **: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n </li>\n <li>\n **lstat **: lower status of the population (percent).\n </li>\n <li>\n **medv **: median value of owner-occupied homes in \\$1000s.\n</ul>\n\n **Source **\nHarrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.\nBelsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n\n\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#1.1Step: Import libraries which are necessary for this project\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom IPython.core.display import display, HTML\n\n\n#1.3.Step: For a nice display in the notebooks\n%matplotlib inline\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#1.4.Step: Preprocessing\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nboston.feature_names \nlower_case = lambda x:x.lower()\nfunc_lower = np.vectorize(lower_case)\nfeature_names = func_lower(boston.feature_names)\nprint(feature_names)\nboston = load_boston()\ndf_boston = pd.DataFrame(boston.data, columns=feature_names)\ndf_boston['medv'] = pd.Series(boston.target)\ndf_boston.head()\n\ndisplay(df_boston.describe())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\ncorr = df_boston.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=100),\n    annot=True, \n    fmt='.2g',\n    square=True,\n\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\nplt.show()\n\ncorrelations = df_boston.corr()\ncorrelations = correlations['medv']\ndisplay(HTML('''\n<p>For the first try the Pearson correlation will be used to measure the corrleation. The Pearson coefficient has values between -1 to 1 </p>\n<ul>\n    <li>0: A value close to zeroe imples a weak correlation. A value of exact 0 impliess no correlation.</li>\n    <li>-1: a value close to -1 implies a strong negativ correlation</li>\n    <li>+1: a value close to +1 implies a strong postiv correlation</li>\n</ul>\n\n'''))\n\ndisplay(\n    correlations.sort_values()\n)\n\n#Step: Selecting only the relevant features\n\nrelevant_feat = correlations[(correlations>0.5) | (correlations<-0.5)]\n\ndisplay(HTML(\"\"\"<h3> Relevant features </h3>\"\"\"))\ndisplay(relevant_feat)\n\ndisplay(\n    HTML(\"\"\"\n        <h3> Finding </h3>\n        You can see that feature's: <br>\n        <ul>\n        <li>lstat  [lower status of the population (percent)] \n        </li><li>\n        rm (average number of rooms)\n        </li><li>\n        ptratio (pupil-teacher ratio by town. )\n        </li>\n        </ul>\n        correlates quite strongly to the medv. However  inorder to be ableu the use Pearson Correlation the <br>\n        the following assumptions need to be checked\n        <ol>\n            <li>continuous variables</li>\n            <li>Cases that have values on both variables</li>\n            <li>Linear relationship between the variables</li>\n            <li>Independent cases (i.e., independence of observations) </li>\n            <li>Bivariate normality </li>\n            <li>Random sample of data from the population</li>\n            <li>No outliers</li>\n        </ol>\n        Source: https://libguides.library.kent.edu/SPSS/PearsonCorr\n        \n        \"\"\"\n        )\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#todo: Write the reasons\n\ndisplay(\n    HTML(\"\"\"\n        <h3> Check  linear assumptions </h3>\n        <ol>\n            <li>continuous variables <br> <b>Fulfilled </b>: </li>\n           \n            <li>Cases that have values on both variables <br> <b>Fulfilled </b></li>\n            <li>Linear relationship between the variable s<br> <b>Fulfilled </b></li>\n            <li>Independent cases (i.e., independence of observations)<br> <b>need to be investigated further </b></li>\n            <li>Bivariate normality <br> <b>need to be investigated further </b></li> </li>\n            <li>Random sample of data from the population <br> <b>Fulfilled </b></li>\n            <li>No outliers <br> <b>need to be investigated further </b> </li>\n        </ol>\n        Source: https://libguides.library.kent.edu/SPSS/PearsonCorr\n        \n        \"\"\"\n        )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\"\"\"\n<h5>Let explore the correlation between the independent variables </h5>\n\"\"\"))\nplt.figure(figsize=(5,5))\ncorr = df_boston[[\"lstat\",\"rm\",\"ptratio\"]].corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=plt.cm.Reds,\n    annot=True, \n    fmt='.2g',\n    square=True,\n\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\nplt.show()\n\n\ndisplay(HTML(\"\"\"\n<h5>Findings</h5>\n<P>As we can see the RM and LSTAT are strongly correlated with each other, <br>\ntherefore I will only keep LSTAT because its correlation coefficient with medv is higher. I will also keep PTRATIO <br>\nbecause the PC value is below 0.5</p>\n\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\n\"\"\"\n<h3>Multilinear regression</h3>\n<p>Prior I identify the most relevant feature to determine the target feature (medv). In this section I will build a \nmultilinear regression, in order to find out how good the identify variable explaine the target var.</p>\n\"\"\"\n))\n\n#todo: Complete this section\n\ndisplay(HTML(\n\"\"\"\n<h3>Multilinear regression</h3>\n\n\n<p>Find a a multilinear model based on: </p>\n<ol>\n    <li>Just use all features \n    \n    </li>\n    <li>Backwards Elimination\n        <ol>\n            <li>Fit model with all possible features</li>\n            <li>Order the features according the P-Value</li>\n            <li>Remove the feature with the highest P-Value, if the p-value is higher than 5%. <br> If\n            no P-Value is higher than 5% the model is finished</li>\n            <li>Refit the model, and go two steps back</li>\n        </ol>\n    </li>\n    <li>Forward Selection\n        <ol>\n            <li>#Uncomplete</li>\n        </ol>\n    </li>   \n    <li>Bidirectional Selection\n        <ol>\n            <li>#Uncomplete</li>\n        </ol>\n    </li>  \n    <li>Sorce Comparision\n        <ol>\n            <li>#Uncomplete</li>\n        </ol>\n    </li>  \n</ol>\n\"\"\"\n))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y = df_boston.iloc[:,-1]\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\n\"\"\"\n<h5>Backwards Elimination </h5>\n\"\"\"))\n\n#Split data into dependent and indepemente var`s\nX = df_boston.iloc[:,:-1]\ny = df_boston[[\"medv\"]]\n#Splitting the dataset into Training and Test set \nfrom sklearn.cross_validation import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n\ndisplay(HTML(\n\"\"\"\n<h5>There is no need to scale the feature var`s because it will be done by the lib because we use multilineare regression</h5>\n\"\"\"))\n\n\n#Fitting the multiple linear regression to the created training data\nimport math \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score,explained_variance_score\n\n\nmodel_all = LinearRegression()\nmodel_all.fit(X_train,y_train)\n\n\n#Evaluate the created model\ny_pred = model_all.predict(X_test)\nplt.style.use('bmh')\n\n#Explore the data\nfig = plt.figure()\n#===== Model with all features \ndf_visu = pd.DataFrame(data=y_pred,columns=[\"predicted\"])\n\ndf_visu_all= pd.concat([\n                        df_visu,y_test.reset_index(drop=True)\n                       ], \n                        axis=1, sort=False\n                       )\n\ndf_visu_all.plot.line(figsize=(20,10),title='Model with all feature',\n                            marker=\".\")\n\nplt.show()\ndisplay(HTML(\n\"\"\"\n<h4>Finding as </h4>\"\"\"))\n# The coefficients\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_pred, y_test))\nprint(\"Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred, y_test)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_pred, y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as sm\n\n\nres = mod.fit()\nprint(res.summary())\n\nfeature_name = df_boston.keys().tolist()\nfeature_name.remove('medv')\nprint(feature_name)\n\nres\nnumVars = len(feature_name)\nsl=0.05\nfor i in range(0, numVars):\n    #1.Step: Building the string\n    str_formular='medv ~'\n    feature_str= ''\n    for element in feature_name:\n        feature_str+=\" + \"+element\n    str_formular+=feature_str\n    print(str_formular.replace(\"~+\",\"~\"))\n    #2.Step:Building model \n    mod = sm.ols(formula=str_formular, data=df_boston)\n    res_back = mod.fit()\n    maxVar=res_back.pvalues.max()\n\n    if maxVar > sl:\n        name_var = res_back.pvalues.idxmax()\n        feature_name.remove(name_var)\n        print(\"#\"*10)\n        print(name_var)\n        print(\"#\"*10)\n    res_back.summary()\n\n\n\n\n#Evaluate the created model\ny_pred_all = model_all.predict(X_test)\ny_pred_select = res.predict(X_test)\nplt.style.use('bmh')\n\n#Explore the data\nfig = plt.figure()\n#===== Model with all features \ndf_visu_all = pd.DataFrame(data=y_pred_all,columns=[\"predicted_all\"])\ndf_visu_select = pd.DataFrame(data=y_pred_select,columns=[\"predicted_select\"])\ndf_visu= pd.concat([\n                        df_visu_all,\n                        df_visu_select.reset_index(drop=True),\n                        y_test.reset_index(drop=True)\n                       ], \n                    axis=1, sort=False\n                   )\n\ndf_visu.plot.line(figsize=(20,10),title='Model with all feature',\n                            marker=\".\")\n\nplt.show()\ndisplay(HTML(\n\"\"\"\n<h4>Finding  </h4>\"\"\"))\n# The coefficients\nprint(\"All: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_all, y_test))\nprint(\"Select: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_select, y_test))\nprint(\"All: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_all, y_test)))\n# Explained variance score: 1 is perfect prediction\nprint(\"Select: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_select, y_test)))\nprint('All: Variance score: %.2f' % r2_score(y_pred_all, y_test))\n# Explained variance score: 1 is perfect prediction\nprint('Select: Variance score: %.2f' % r2_score(y_pred_select, y_test))\n\n\ndisplay(HTML(\n\"\"\"\n<h5>As we can see, even if we use multply linear regression the result is still quite bad, <br>\nthis could mean that the is some non linear correlation </h5>\"\"\"))\n\n## On","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_self ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\n\"\"\"\n<h5>Now I will build a multi. linear model with just the identify feature </h5>\"\"\"))\n\nmod_self = sm.ols(formula=\"medv ~ lstat +  ptratio \", data=df_boston)\nres_self = mod_self.fit()\nres_self.summary()\n\n\n\n\n#Evaluate the created model\ny_pred_all = model_all.predict(X_test)\ny_pred_select = res.predict(X_test)\ny_pred_self = res_self.predict(X_test)\nplt.style.use('bmh')\n\n#Explore the data\nfig = plt.figure()\n#===== Model with all features \ndf_visu_all = pd.DataFrame(data=y_pred_all,columns=[\"predicted_all\"])\ndf_visu_select = pd.DataFrame(data=y_pred_select,columns=[\"predicted_select\"])\ndf_visu_self = pd.DataFrame(data=y_pred_self,columns=[\"predicted_self\"])\ndf_visu= pd.concat([\n                        df_visu_all,\n                        df_visu_self.reset_index(drop=True),\n                        df_visu_select.reset_index(drop=True),\n                        y_test.reset_index(drop=True)\n                       ], \n                    axis=1, sort=False\n                   )\n\ndf_visu.plot.line(figsize=(20,10),title='Model with all feature',\n                            marker=\".\")\n\nplt.show()\ndisplay(HTML(\n\"\"\"\n<h4>Finding  </h4>\"\"\"))\n# The coefficients\nprint(\"All: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_all, y_test))\nprint(\"Select: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_select, y_test))\nprint(\"Self: Mean squared error: %.2f\"\n      % mean_squared_error(y_pred_self, y_test))\nprint(\"All: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_all, y_test)))\n# Explained variance score: 1 is perfect prediction\nprint(\"Select: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_select, y_test)))\nprint(\"Self: Standard deviation: %.2f\"\n      % math.sqrt(mean_squared_error(y_pred_self, y_test)))\nprint('All: Variance score: %.2f' % r2_score(y_pred_all, y_test))\n# Explained variance score: 1 is perfect prediction\nprint('Select: Variance score: %.2f' % r2_score(y_pred_select, y_test))\nprint('Self: Variance score: %.2f' % r2_score(y_pred_self, y_test))\n\ndisplay(HTML(\n\"\"\"\n<h5>As we can see, even if we use multply linear regression the result is still quite bad, <br>\nthis could mean that the is some non linear correlation </h5>\"\"\"))\n\n## On","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"display(HTML(\"\"\"\n<h5>Letâ€™s use pandas to visualise this correlation further</h5>\n\"\"\"\n))\n\n#Explore the data\nplt.subplots_adjust(left=0.2, bottom=None, right=None, top=None, wspace=None, hspace=1)\nplt.subplot(211)\n#===== AGE\nplt.subplot(2, 1, 1)\ndf_boston['age'].plot.hist(title='Proportion of owner-occupied units built prior to 1940',\n                            colormap='jet')\nplt.xlabel('age')\nplt.ylabel('frequency')\n#===== \nplt.subplot(2, 1, 2)\ndf_boston['crim'].plot.hist(title=\"per capita crime rate by town\",\n                           colormap=\"jet\"\n                           )\nplt.xlabel(\"crime\")\nplt.ylabel(\"frequency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom pandas.plotting import scatter_matrix\n\ncolumns_visual=['rm','lstat','ptratio']\n\nfor col_name in columns_visual:\n    plt.figure()\n    df_boston.plot.scatter(x=col_name, y='medv', title=\"Medv and {col_name}\".format(col_name=col_name))\n\n\n\nscatter_matrix(df_boston[columns_visual],figsize=(15,12),alpha=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creation a prediciton model\n\nIn order to mesaure the quality of the developed model I will you RÂ². The Range of this coefficient  is between 0 to 1. \n\n- 0: A model with a RÂ² is as good as a model which always predict the mean of a traget variable.\n- 1: The developed model can perfely predict the target varaible\n- 0-1: Can be see as an indicator, what a percentag the target variable can be explaind by the feature which this model uses. \n  "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Import 'r2_score'\n\nfrom sklearn.metrics import r2_score\n\n# Split date into training and test data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_boston[columns_visual], df_boston['medv'], test_size=0.2, random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pydotplus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit\n\ndef fit_model(X, y):\n    \"\"\" Performs grid search over the 'max_depth' parameter for a \n        decision tree regressor trained on the input data [X, y]. \n    Source:https://towardsdatascience.com/machine-learning-project-predicting-boston-house-prices-with-regression-b4e47493633d     \n    \"\"\"\n    \n    # Create cross-validation sets from the training data\n    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n\n    # Create a decision tree regressor object\n    regressor = DecisionTreeRegressor()\n\n    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n    params = {'max_depth':[1,2,3,4,5,6,7,8,9,10]}\n\n    # Transform 'performance_metric' into a scoring function using 'make_scorer' \n    scoring_fnc = make_scorer(r2_score)\n\n    # Create the grid search cv object --> GridSearchCV()\n    grid = GridSearchCV(estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_,grid\n\n# Fit the training data to the model using grid search\nreg,grid = fit_model(X_train, y_train)\n\n# Produce the value for 'max_depth'\nprint(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"grid.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create test data to explore the created model \n\n#             rm ,lstat,ptratio   \nimaginary_data = [[2, 15, 20], \n               [3, 25, 26],\n               [6, 5, 9]]  \n\n# Show predictions\nfor i, price in enumerate(reg.predict(imaginary_data)):\n    print(\"Predicted selling price for test data {}'s : ${:,.2f}\".format(i+1, price))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\n\n# https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176\ndot_data = StringIO()\n\nexport_graphviz(reg, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelâ€™s Sensitivity\n\nAfter I found an optimal model I will check the robustness of the developed model. \nIt can happen that the optional model is overfiting order underfitting. \n\nOne way to check if the model is robistness of a concept is to created differnt models and see how far the spread is for the same data set ( I will check agains the imaginary_data).\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.Step Change the test_split to \ni=0\ndict_model={}\ndf_sens= pd.DataFrame(data={\"name\":[],\"predict_value\":[]})\nfor size_test_split in [0.1,0.15,0.22,0.27]:\n    \n    X_train, X_test, y_train, y_test = train_test_split(df_boston[columns_visual], df_boston['medv'], test_size=0.2, random_state = 42)\n    reg,grid = fit_model(X_train, y_train)\n    dict_model[i]={\"reg\":reg,\"grid\":grid}\n    df_sens = df_sens.append({\"name\":i,\"predict_value\":reg.predict([[2, 15, 20]])[0]},ignore_index=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df_sens.append({\"name\":i,\"predict_value\":reg.predict([[2, 15, 20]])[0]},ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_sens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_boston.plot.scatter(x=df_sens[\"predict_value\"], y=df_sens[\"predict_value\"], title=\"Medv and rm\")\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python (python35)","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"}},"nbformat":4,"nbformat_minor":1}